"""
FastAPI Application.
Exposes REST API for the conversational AI system.

Design principle: Stateless API with session management via thread_id.
Ready for containerization and horizontal scaling.
"""
import logging
from typing import List, Optional
from uuid import uuid4

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain_core.messages import HumanMessage

from config import settings
from state import create_initial_state, ConversationState
from graph import create_support_graph, visualize_graph

# Configure logging
logging.basicConfig(
    level=settings.log_level,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialize FastAPI
app = FastAPI(
    title="Telecom Support AI",
    description="Multi-agent conversational AI for technical and billing support",
    version="1.0.0"
)

# CORS (adjust for production)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Restrict in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize graph globally (singleton)
support_graph = create_support_graph()
logger.info("‚úÖ Application started - support graph ready")


# Request/Response models
class ChatRequest(BaseModel):
    """Request schema for chat endpoint."""
    message: str
    user_id: str
    thread_id: Optional[str] = None  # For conversation continuity


class ChatResponse(BaseModel):
    """Response schema for chat endpoint."""
    response: str
    thread_id: str
    agent: str
    category: Optional[str] = None


class HealthResponse(BaseModel):
    """Response schema for health check."""
    status: str
    model: str
    vectorstore: str


# Endpoints
@app.get("/", response_model=dict)
async def root():
    """Root endpoint with API info."""
    return {
        "service": "Telecom Support AI",
        "version": "1.0.0",
        "endpoints": {
            "POST /chat": "Send a message to the support system",
            "GET /health": "Health check",
            "GET /graph": "Visualize conversation graph"
        }
    }


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """
    Health check endpoint.
    Verifies OpenAI connection and vector store availability.
    """
    try:
        # Basic health indicators
        return HealthResponse(
            status="healthy",
            model=settings.openai_model,
            vectorstore=settings.vector_store_type
        )
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(status_code=503, detail="Service unhealthy")


@app.get("/graph")
async def get_graph_visualization():
    """
    Return Mermaid diagram of conversation graph.
    Useful for documentation and debugging.
    """
    diagram = visualize_graph(support_graph)
    if diagram:
        return {"mermaid": diagram}
    else:
        return {"message": "Graph visualization not available"}


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Main chat endpoint.
    Processes user message through the multi-agent system.
    
    Args:
        request: ChatRequest with message, user_id, and optional thread_id
    
    Returns:
        ChatResponse with agent response and metadata
    """
    try:
        logger.info(f"Incoming message from user {request.user_id}: {request.message[:100]}...")
        
        # Generate or reuse thread_id for conversation continuity
        thread_id = request.thread_id or str(uuid4())
        
        # Create conversation config (for memory persistence)
        config = {
            "configurable": {
                "thread_id": thread_id
            }
        }
        
        # Create initial state for this turn
        initial_state = create_initial_state(
            user_id=request.user_id,
            initial_message=HumanMessage(content=request.message)
        )
        
        # Execute graph
        logger.info(f"Executing graph for thread {thread_id}")
        result = support_graph.invoke(initial_state, config)
        
        # Extract response
        last_ai_message = None
        for msg in reversed(result["messages"]):
            if msg.type == "ai":
                last_ai_message = msg
                break
        
        if not last_ai_message:
            raise ValueError("No AI response generated")
        
        response_text = last_ai_message.content
        agent_name = result.get("last_agent", "unknown")
        category = result.get("current_category")
        
        logger.info(f"‚úì Response generated by {agent_name} ({len(response_text)} chars)")
        
        return ChatResponse(
            response=response_text,
            thread_id=thread_id,
            agent=agent_name,
            category=category
        )
        
    except Exception as e:
        logger.error(f"Chat endpoint error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Internal error processing message: {str(e)}"
        )


@app.post("/reset")
async def reset_conversation(thread_id: str):
    """
    Reset a conversation thread.
    Clears memory for the given thread_id.
    
    Args:
        thread_id: Thread identifier to reset
    
    Returns:
        Success confirmation
    """
    try:
        # Note: MemorySaver doesn't have explicit clear method
        # In production, implement proper session management
        logger.info(f"Reset requested for thread {thread_id}")
        return {"message": f"Conversation {thread_id} reset (restart with new thread_id)"}
    except Exception as e:
        logger.error(f"Reset error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# CLI mode (for testing without API)
def cli_mode():
    """
    Interactive CLI for testing the support system.
    Run with: python main.py --cli
    """
    print("=" * 60)
    print("ü§ñ Telecom Support AI - CLI Mode")
    print("=" * 60)
    print("Type 'quit' to exit\n")
    
    thread_id = str(uuid4())
    user_id = "cli_user"
    
    while True:
        try:
            user_input = input("You: ").strip()
            
            if user_input.lower() in ["quit", "exit"]:
                print("Goodbye!")
                break
            
            if not user_input:
                continue
            
            # Create state
            initial_state = create_initial_state(
                user_id=user_id,
                initial_message=HumanMessage(content=user_input)
            )
            
            config = {"configurable": {"thread_id": thread_id}}
            
            # Execute
            result = support_graph.invoke(initial_state, config)
            
            # Extract response
            for msg in reversed(result["messages"]):
                if msg.type == "ai":
                    print(f"\nü§ñ Agent ({result['last_agent']}): {msg.content}\n")
                    break
                    
        except KeyboardInterrupt:
            print("\n\nGoodbye!")
            break
        except Exception as e:
            print(f"‚ùå Error: {e}\n")


if __name__ == "__main__":
    import sys
    
    if "--cli" in sys.argv:
        cli_mode()
    else:
        import uvicorn
        uvicorn.run(app, host="0.0.0.0", port=8000)
